### 联邦学习分类：

根据多参与方之间数据分布的不同，把联邦学习分为三类。

![preview](https://pic3.zhimg.com/v2-2f90ef909f950e2e83e5bd9aad4f5432_r.jpg)

#### 横向联邦学习

横向联邦学习的本质是**样本的联合**，适用于参与者间业态相同但触达客户不同。

**即特征重叠多（特征相似），用户重叠少（样本不同）。**

如：不同地区的银行间，他们需要的特征一样，但需要用户不同（样本不同）。

![img](https://pic4.zhimg.com/v2-23616816b92a6d62be206b0aa28ba393_b.jpg)



#### 纵向联邦学习

纵向联邦学习的本质是**特征的联合**，**适用于用户重叠多，特征重叠少的场景。**

如：同一地区的商超和银行，拥有同一个用户（样本相同）的不同特征。

<img src="https://pic4.zhimg.com/v2-3d3a6fbef04273a7729364789f0902fb_b.jpg" alt="img"  />

**在整个过程中参与方都不知道另一方的数据和特征，且训练结束后参与方只得到自己侧的模型参数，即半模型。**利用对方数据，提升自己模型。



#### 联邦迁移学习

**当参与者间特征和样本重叠都很少时**可以考虑使用联邦迁移学习，如不同地区的银行和商超间的联合。主要适用于以深度神经网络为基模型的场景。

**通俗迁移学习思想：**其实我们人类对于迁移学习这种能力，是与生俱来的。比如，我们如果已经会打乒乓球，就可以类比着学习打网球。再比如，我们如果已经会下中国象棋，就可以类比着下国际象棋。因为这些活动之间，往往有着极高的相似性。生活中常用的“举一反三”、“照猫画虎”就很好地体现了迁移学习的思想。**迁移学习的核心是，找到源领域和目标领域之间的相似性。**



### Non-IID数据

#### 分类：

**Feature distribution skew	特征差异P(x)**

​		*因为用户之间喜好和生活习惯不同，因此产生了数据分布的不一致性。*

​		举例来说，在mnist数据集，客户 i 喜欢并经常将数字 5 写成细字体，而客户 j 喜欢并经常将数字 5 写成粗体，但是字体都是 5。

**Label distribution skew	标签差异P(y)**

​		*举例来说，某个client对 1，3，5数字各有100个图片，另一个client 3较多，300个图片几乎都是数字3)*

**Quantity skew	数量差异**

#### 非同分布（一般指这个）：

​	例如，美国西海岸针叶林茂盛，东海岸阔叶林分布广泛，又因为用户之间喜好和生活习惯不同，因此产生了数据分布的不一致性; 

​	例如，在mnist数据集中，有的设备只含1，3，5数据、有的设备含0-9所有数据、有的设备只要2.

#### 非独立

​	另一方面，以家庭或亲属关系的群体之间会相互影响，产生了数据分布的不独立性。

#### IID构造

将所有数据均匀地分布在N个客户端。

同程度的NonIID主要取决于每个客户端所见到类别的数量，每个客户端上面只有一个类别的数据自然是最NonIID的情形，每个客户端上有所有类别的数据就是IID的设置。

如：在mnist中，每个设备有全部的类别数字数据，每个数字数据数量一样。

#### 异构程度的衡量

**第一种方法：**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20201010182300324.png#pic_center)

式子具体化为：

![image-20220702165837543](C:\Users\10275\AppData\Roaming\Typora\typora-user-images\image-20220702165837543.png)

F为全局目标函数，Fk为第k个客户端的目标函数。

即使用一个参数的全局模型的目标函数与使用n个参数的本地模型的目标函数之和 的差

若是iid数据，T接近=0。*异构程度越大，T越大。*

*参考文献：On the convergence of fedavg on non-iid data*

**第二种方法：**

概率q分配，来衡量异构程度。下面是mnist例子：

MNIST有60,000个训练示例和10,000个测试示例。我们考虑n=1000个客户，我们将他们分成10组。我们将一个标签为 l 的训练示例分配给概率为q的第 l 组客户，并以概率为 (1−q)/9 将其分配给剩余的每一组。在为一个组分配一个训练示例之后，我们将其均匀随机地分配给组中的一个客户端。参数q控制着客户端上的局部训练数据分布，我们称q度为非IID。q=0.1表示客户的本地训练数据为IID，而q越大表示非IID的程度越大。默认情况下，我们设置q=为0.5。

*参考文献：Provably Secure Federated Learning against Malicious Clients*

**第三种方法：**

![image-20221107170628372](https://raw.githubusercontent.com/LifeSum12/typora-image/main/img/202211071706935.png)

*参考文献：*



#### non-iid解决方案

基于数据：

* 数据共享
* 数据提升
* **数据选择（用户选择）**

​		FedCS：根据用户计算能力等信息，来选择每轮参与的客户端，但开展前需要用户先上传本地资源信息。

​		FedSS：若每个用户计算能力一致，数据量大小不同，则本地训练时间不同。设置一个数据量量标准，当用户数据量小于这个标准则正常训练，数据量当大于这个标准，则进行采样训练（尽量挑有用的数据集）。

​		数据集分布度量方法：

​						1. 卡方检验	2.交叉熵	3.相对熵（KL散度）	4. js散度



基于模型：

* 模型更新和聚合
* 动态优化
* 正则优化







### 分布式学习、Cross-silo、Cross-device区别

Cross-device 是由大量不可靠设备（不可靠指网络不稳定，数据可能有问题）

Cross-silo 是由小量可靠设备

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly95Zi10ZXN0MDEub3NzLWNuLXNoZW56aGVuLmFsaXl1bmNzLmNvbS9pbWcvMjAyMDA4MzExMTEwMzgucG5n?x-oss-process=image/format,png)



### 联邦学习特性

**1.通信是联邦设置中的一个关键瓶颈。**联合网络可能由大量设备组成，网络中的通信可能比本地计算慢许多数量级。来自《Advances and open problems in federated learning》

**2.有偏差的设备参与（fedavg允许）。**每个设备参与训练的次数不相等，这种有偏见的设备参与使得全局模型偏向于一些更频繁参与的设备。



### 个性化联邦学习

分类：

![img](https://raw.githubusercontent.com/LifeSum12/typora-image/main/img/202212221432347.webp)

#### 一、全局个性化模型

​	目标：解决non-IID数据

​	第一阶段，训练一个共享的全局FL模型；第二阶段，在本地的数据上进行额外的训练，达到适应个性化的目的。

#### 	①**基于数据的方法**

##### 			Ⅰ数据增强

​			通过在云端建立proxy dataset 或者通过每个数据中心分享数据获得对整体数据分布的估计。

​			问题研究：训练集和测试集的数据分布差异是造成联邦学习准确性下降的原因之一，若分布一致可以恢复iid模型的精度。

> - Zhao，Li等人提出了一种数据共享策略，该策略将少量的全局数据按类别平衡分配给每个客户端。
> - Duan，liu等人提出了Astraea，一个自平衡的FL框架，通过使用基于z分数的数据增强和本地数据的下采样来处理类的不平衡。FLserver需要关于客户端本地数据分布的统计信息（例如，类大小、平均值和标准偏差值）。				

##### 			Ⅱ client选择；

​			使其能够从更均匀的数据分布中进行采样，目的是提高模型的泛化性能。

#### 	②**基于模型的方法**

​			正则化；Fedprox、SCAFFOLD等，修改loss函数。

​			元学习（meta learning）；

​			迁移学习（transfer learning）；先训练一个全局FL模型，在本地数据上训练一个本地模型，最后通过全局FL模型对于本地模型精调。其目标是降低本地模型和全局FL模型之间的不一致性。

#### 二、学习个性化模型

​	目标：达成模型个性化

​	 **基于结构的方法**

​			参数分解（分层）；

> - 第一种是"**基础层+个性化层**"设计。在这种设置中，个性化的深层被客户保留在本地训练中，以学习个性化的特定任务表征，而基础层则与FL客户端共享，以学习低层次的通用特征。
> - 

​			知识蒸馏；

​	**基于相似性的方法**

​			多任务学习；

​			模型差值；

​			聚类：将数据中心之间聚类，并且对每一个类训练一个模型。

## 论文

### 算法

#### FedProx

​	loss函数加 正则项。动态本地训练次数。

#### Scaffold

​	w = w- η*g 更新时候加入参数C，w=w- η(g- ci+c)

​	通过参数C，控制每次本地更新走向。但需要增加通信量。



### 个性化

#### L2GD

loss函数加正则项。通过概率p控制本地更新还是服务器聚合。

#### FedOpt

提出了一个框架。即本地更新、全局更新，可以使用不用的优化器。（如adam等）

#### Ditto

​	本地先按照FedAvg的算法进行优化，然后再加上一个正则项继续优化本地模型，最后传给服务器的是没有加正则项的优化结果，就是Local和Global两不耽误。

#### FedRep

大部分网络结构（全局表示） 用来进行全局模型，需要上传聚合。

小部分网络结构 （客户端头）用来本地更新，不上传仅在本地更新。

最终的模型是=全局表示+客户端头。

#### FedFomo

提出服务端保存每个客户的模型，且客户从服务端下载的不再是全局统一模型，是特定客户端模型的组合。（被选中的特定客户端对该客户是有利的）。

新的权重公式，并且使用概率p减少通信。



### 模型网络修改（未细读）

#### FedBN 

解决异构数据中的 feature shift。

#### HeteroFL

降低本地客户端的计算和通信复杂度。



### 其他

#### **AsyncCommSGD**

异步联邦学习。解决设备不稳定，non-iid，收敛慢的问题。



### 数据异构

FedMix

对数据相互进行平均混合，降低iid影响。

